{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079ae47a",
   "metadata": {},
   "source": [
    "<img style=\"max-width:20em; height:auto;\" src=\"../graphics/A-Little-Book-on-Adversarial-AI-Cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc332e",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne   \n",
    "Author Blog: https://www.securitynik.com   \n",
    "Author GitHub: github.com/securitynik   \n",
    "\n",
    "Author Other Books: [   \n",
    "\n",
    "            \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\",   \n",
    "            \n",
    "            \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\"   \n",
    "        ]   \n",
    "\n",
    "\n",
    "This notebook ***(hop_skip_jump_white-box.ipynb)*** is part of the series of notebooks From ***A Little Book on Adversarial AI***  A free ebook released by Nik Alleyne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312a54d",
   "metadata": {},
   "source": [
    "### HopSkipJumpAttack with ART: White-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba13af7",
   "metadata": {},
   "source": [
    "### Lab Objectives:   \n",
    "- Understanding of HopSkipJumpAttack      \n",
    "- Use Adversarial Robustness Toolkit (ART) to perform HopSkipJumpAttack  \n",
    "- Perform the HopSkipJumpAttack when you have the model file (white-box)     \n",
    "- Leverage mlflow    \n",
    "\n",
    "**Keep the MLflow infrastructure running at the end. The other two labs are dependent on this infrastructure**  \n",
    "\n",
    "\n",
    "### Step 1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666d378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Get the ART libraries  \n",
    "import art\n",
    "from art.attacks.evasion import HopSkipJump\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769ec5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version used:  2.7.1+cu128\n",
      "Torchvision version used:  0.22.1+cu128\n",
      "ART version used:  1.20.1\n",
      "Numpy version used:  2.1.3\n",
      "MLflow version used:  3.1.1\n"
     ]
    }
   ],
   "source": [
    "### Version of key libraries used  \n",
    "print(f'Torch version used:  {torch.__version__}')\n",
    "print(f'Torchvision version used:  {torchvision.__version__}')\n",
    "print(f'ART version used:  {art.__version__}')\n",
    "print(f'Numpy version used:  {np.__version__}')\n",
    "print(f'MLflow version used:  {mlflow.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf45275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the device to cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to work with\n",
    "# This should ensure if there are accelerators in place, such as Apple backend or CUDA, \n",
    "# we should be able to take advantage of it.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Setting the device to cuda')\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('Setting the device to Apple mps')\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print('Setting the device to CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12b97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a transform for the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Get the train and test data from MNIST\n",
    "train_dataset = datasets.MNIST(root=r'/tmp/', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=r'/tmp/', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d88f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a train loader\n",
    "batch_size=64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1_000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1d89d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a bach of the data\n",
    "X_sample, y_sample = next(iter(train_loader))\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2adde65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 4, 7, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does y look like\n",
    "y_sample[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8b3b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1, 28, 28]), torch.Size([1000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the test data\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3072f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the min and max value\n",
    "X_test.min(), X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa97ae",
   "metadata": {},
   "source": [
    "### Start the mlflow server    \n",
    "~$ **mlflow server --host 127.0.0.1 --port 9999**      \n",
    "[2025-06-22 10:59:45 -0400] [191683] [INFO] Starting gunicorn 23.0.0  \n",
    "[2025-06-22 10:59:45 -0400] [191683] [INFO] Listening at: http://127.0.0.1:9999 (191683)   \n",
    "[2025-06-22 10:59:45 -0400] [191683] [INFO] Using worker: sync   \n",
    "\n",
    "With the server running, open your browser and go to: http://127.0.0.1:9999     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d1e03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/118242476423976612', creation_time=1752378317737, experiment_id='118242476423976612', last_update_time=1752378317737, lifecycle_stage='active', name='HopSkipJumpAttack', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the mlflow tracking server \n",
    "# mlflow requires us to have a tracking URL\n",
    "tracking_uri = 'http://127.0.0.1:9999'\n",
    "mlflow.set_tracking_uri(uri=tracking_uri)\n",
    "\n",
    "# Create an experiment\n",
    "# We name our experiment HopSkipJumpAttack   \n",
    "mlflow.set_experiment(experiment_name='HopSkipJumpAttack')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b45333",
   "metadata": {},
   "source": [
    "Get some data to use with our simple model.  Remember, neither the data or network is important here. The only thing that matters is our ability to attack the model. We are also building the model, so we can see everything from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926f697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ccfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0072, -0.0310, -0.0320,  0.0058, -0.0486, -0.0304, -0.0065, -0.0228,\n",
       "          0.0595, -0.0317],\n",
       "        [-0.0210, -0.0867, -0.0217,  0.0462, -0.0582, -0.0198,  0.0008, -0.0172,\n",
       "          0.0002, -0.0585],\n",
       "        [-0.0164, -0.0377, -0.0165,  0.0373, -0.0787, -0.0454, -0.0119, -0.0077,\n",
       "          0.0397, -0.0525],\n",
       "        [-0.0125, -0.0473, -0.0116,  0.0150, -0.0491, -0.0363,  0.0262,  0.0115,\n",
       "         -0.0043, -0.0720],\n",
       "        [-0.0306, -0.0666, -0.0344, -0.0042, -0.0191, -0.0371, -0.0366, -0.0237,\n",
       "          0.0482, -0.0292]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random number generator for reproducibility \n",
    "torch.random.manual_seed(10)\n",
    "\n",
    "# Create a simple model \n",
    "# Nothing exciting about our architecture \n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1, bias=False),\n",
    "    nn.MaxPool2d(kernel_size=(3,3), stride=1, padding=1),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1),\n",
    "    nn.MaxPool2d(kernel_size=(3,3), stride=1, padding=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=50176, out_features=1024),\n",
    "    nn.Linear(in_features=1024, out_features=10)\n",
    "    #nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device=device)\n",
    "model(X_sample.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62414f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 3, 8, 6, 8, 8, 3, 8, 3, 8, 3, 3, 3, 3, 3, 6, 8, 8, 3, 8, 6, 8, 3, 8,\n",
       "        8, 3, 3, 7, 8, 3, 6, 3, 3, 8, 3, 8, 3, 3, 3, 6, 8, 3, 8, 3, 8, 3, 3, 3,\n",
       "        8, 8, 3, 3, 3, 3, 8, 8, 8, 6, 3, 3, 3, 3, 6, 3], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us get the predictions\n",
    "model(X_sample.to(device)).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9b7f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e4e00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:12<00:00, 72.93it/s] \n",
      "Epoch: 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 61.19it/s]\n",
      "Epoch: 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:12<00:00, 72.66it/s]\n",
      "Epoch: 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:28<00:00, 33.15it/s]\n",
      "Epoch: 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:32<00:00, 29.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Test accuracy: 0.9539999961853027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/01 23:17:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/01 23:17:51 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "Registered model 'mnist_registered_model' already exists. Creating a new version of this model...\n",
      "2025/08/01 23:17:52 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mnist_registered_model, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in run 8027d3d03137407f83dc10f53cb02fce\n",
      "Model URL: runs:/8027d3d03137407f83dc10f53cb02fce/mnist_model\n",
      "ðŸƒ View run final_run at: http://127.0.0.1:9999/#/experiments/118242476423976612/runs/8027d3d03137407f83dc10f53cb02fce\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:9999/#/experiments/118242476423976612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'mnist_registered_model'.\n"
     ]
    }
   ],
   "source": [
    "# Define an epoch\n",
    "epoch = 5\n",
    "\n",
    "# Setup a loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss ()\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.1)\n",
    "\n",
    "# Wrap the training loop inside of mlflow context manager\n",
    "with mlflow.start_run(run_name='final_run') as final_run:\n",
    "    # Setup a model signature for the input and output of the model\n",
    "    signature = infer_signature(model_input=X_sample.numpy(), model_output=model(X_sample.to(device)).detach().cpu().numpy()) \n",
    "    \n",
    "    # Log the learning rate\n",
    "    mlflow.log_param('lr', 0.1)\n",
    "\n",
    "    # Run our training loop\n",
    "    for i in range(epoch):\n",
    "        for X, y in tqdm(train_loader, desc=f'Epoch: {i+1}/{epoch}'):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    print('Training completed!')\n",
    "\n",
    "    # Just looking at the test accuracy. \n",
    "    # Training accuracy is not important for the problem we are solving\n",
    "    test_logits = model(X_test.to(device))\n",
    "    test_accuracy = sum(test_logits.detach().cpu().argmax(dim=-1) == y_test) / y_test.shape[0]\n",
    "    print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "    model_info = mlflow.pytorch.log_model(pytorch_model=torch.jit.script(model), name='mnist_model', signature=signature, registered_model_name='mnist_registered_model')\n",
    "    \n",
    "    model_uri = f'runs:/{final_run.info.run_id}/mnist_model'\n",
    "\n",
    "    print(f'Model saved in run {final_run.info.run_id}')\n",
    "    print(f'Model URL: {model_uri}')\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56826787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9540)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the trained model's accuracy\n",
    "test_logits = model(X_test.to(device))\n",
    "test_accuracy = sum(test_logits.detach().cpu().argmax(dim=-1) == y_test) / y_test.shape[0]\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d8a9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# We can save in either approach\n",
    "torch.save(model.state_dict(), '/tmp/model.pth')\n",
    "torch.jit.script(model).save(f='/tmp/model.jit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa7a44",
   "metadata": {},
   "source": [
    "### Step 2:  \n",
    "With the model saved, we can now grab the model URI.  \n",
    "\n",
    "**Note:** You are more likely to get a different value for your models URI than I have above. Please replace the URI with that below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86a45d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model URI is: models:/m-2560fbf839e3419f8eee1e5a8aebc7b5\n"
     ]
    }
   ],
   "source": [
    "# Getting the model URI\n",
    "model_uri = model_info.model_uri\n",
    "print(f'The Model URI is: {model_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616318d",
   "metadata": {},
   "source": [
    "Split your terminal window if you can into two halves. This allows you to run things side-by-side so you can see everything within one screen.   \n",
    "\n",
    "*Set environment variable*    \n",
    "Before moving on, open another terminal and export the mlflow tracking URI\n",
    "$ **export MLFLOW_TRACKING_URI=http://localhost:9999**    \n",
    "\n",
    "You can then verify the variable is et:   \n",
    "$ **env | grep MLFLOW**   \n",
    "MLFLOW_TRACKING_URI=http://localhost:9999\n",
    "\n",
    "*Load up the inference interface*    \n",
    "**Note:** You are more likely to get a different value for your models URI than I have above. Please replace the URI with that below.  \n",
    "$ **mlflow models serve --model-uri models:/m-bccb3b44d33c44349ea980b9fd975766 --port 5000 --no-conda**   \n",
    "....    \n",
    "INFO:     Started server process [192157]    \n",
    "INFO:     Waiting for application startup.    \n",
    "INFO:     Application startup complete.    \n",
    "INFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)     \n",
    "\n",
    "\n",
    "### Step 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6915eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2eff4e9e1e4825900b2de651c7135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4330867e+08, -7.3795824e+07,  5.5985845e+06, -9.3815560e+07,\n",
       "        -9.9743872e+07, -2.0231716e+07,  5.1758175e+06, -5.3083552e+07,\n",
       "         9.5131470e+06, -3.5361936e+07],\n",
       "       [-7.7427576e+07,  2.7723958e+07,  1.1101824e+08, -3.4468980e+07,\n",
       "        -4.9028880e+07, -1.2028488e+08, -1.3567206e+08,  3.7724292e+07,\n",
       "         6.3831540e+06, -3.0080378e+07],\n",
       "       [-1.9049606e+07, -7.7796016e+07, -6.4887908e+07, -2.4867106e+07,\n",
       "        -7.9246320e+07,  9.8317048e+07, -3.6657536e+07, -7.0054552e+07,\n",
       "        -2.2677350e+07, -3.3195714e+07],\n",
       "       [-1.5234531e+08, -1.2606442e+08, -7.3199576e+07,  1.0584658e+08,\n",
       "        -1.5190426e+08, -1.5963080e+07, -2.3739141e+08, -4.1452616e+07,\n",
       "        -3.1128140e+07, -3.7700292e+07],\n",
       "       [-6.4920584e+07, -6.7215512e+07, -3.5828904e+07, -1.0997389e+07,\n",
       "         8.7548656e+07, -3.6595648e+06, -1.0949787e+08,  6.8353376e+07,\n",
       "         2.0529970e+07,  9.6061064e+07]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model\n",
    "# This will work but throws an error about torch.jit.load\n",
    "# This is because we save the model above via torch.jit.script(model)\n",
    "# Not a problem, we get our predictions.  \n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "loaded_model.predict(X_test.numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c264c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b315e9d78c04e1e8e775910f474a229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local model path is: /tmp/tmpb0rnssuk/\n",
      "MLmodel     data\t     registered_model_meta\n",
      "conda.yaml  python_env.yaml  requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Let us try to remove that warning\n",
    "# First let's find where the model is saved\n",
    "local_model_path = mlflow.artifacts.download_artifacts(artifact_uri=model_uri)\n",
    "print(f'The local model path is: {local_model_path}')\n",
    "\n",
    "# Now that we have the directory \n",
    "# If we list the directory we see \n",
    "\n",
    "!ls {local_model_path}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8754517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth  pickle_module_info.txt\n"
     ]
    }
   ],
   "source": [
    "# Notice teh data folder above?\n",
    "# This is where our model is stored  \n",
    "# Prepended the data directory at the end of our path\n",
    "# Also note, the model file is model.pth\n",
    "!ls {local_model_path}/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ee39a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (2): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (3): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (4): RecursiveScriptModule(original_name=Flatten)\n",
       "  (5): RecursiveScriptModule(original_name=Linear)\n",
       "  (6): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model finally\n",
    "loaded_model = torch.jit.load(f=f'{local_model_path}/data/model.pth')\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c359e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 5, 3, 9], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With our final model, make some predictions\n",
    "loaded_model(X_test.to(device)[:5]).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cea27",
   "metadata": {},
   "source": [
    "### Step 4:  \n",
    "Now that we know this works, let us, let us use this model via a white-box approach for the HopSkipJump Attack    \n",
    "\n",
    "Note: **Unfortunately, when I tried to build the ART classifier I started running into error. These errors are being left here for you to see the problem and recognize, building these solutions are not always simply plug and play but requires testing. More importantly, I am leaving the errors here for us to learn from them.**\n",
    "\n",
    "For some strange reason, ART was seeing float64 values.    \n",
    "Here is an error \"\"\" RuntimeError: expected scalar type Double but found Float \"\"\"    \n",
    "\n",
    "While most of the data seems to be float32, somehow, parts of the data seems to be flaot64 ...\n",
    "\"\"\"   \n",
    "[DEBUG] input dtype: torch.float32   \n",
    "[DEBUG] input dtype: torch.float32   \n",
    "....   \n",
    "[DEBUG] input dtype: torch.float32   \n",
    "[DEBUG] input dtype: torch.float64    \n",
    " \n",
    "\"\"\"   \n",
    "\n",
    "Notice the sudden change to float64. I am not sure why this was the case. However, this code below, helped me to detect this.   \n",
    "\n",
    "Notice below, the wrapped_model is not set as model. You would need to make this change further below.    \n",
    "art_white_box_clf = PyTorchClassifier(   \n",
    "    model=wrapped_model,    \n",
    "    loss=torch.nn.CrossEntropyLoss(),    \n",
    "    optimizer=torch.optim.Adam(params=wrapped_model.parameters(), lr=0.1),    \n",
    "    clip_values=(0., 1.),    \n",
    "    nb_classes=10,    \n",
    "    input_shape=X_test.shape[1:],    \n",
    "    device_type=device    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "065e3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was having problem with the model\n",
    "class DebugModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f'[DEBUG] input dtype: {x.dtype}')\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "767c52a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebugModel(\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "    (1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "    (2): RecursiveScriptModule(original_name=Conv2d)\n",
       "    (3): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "    (4): RecursiveScriptModule(original_name=Flatten)\n",
       "    (5): RecursiveScriptModule(original_name=Linear)\n",
       "    (6): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the wrapped model for debugging \n",
    "wrapped_model = DebugModel(model=loaded_model)\n",
    "wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb75ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the classifier \n",
    "art_white_box_clf = PyTorchClassifier(\n",
    "    model= wrapped_model,    # Notice the wrapped model here. This is for debugging below.\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(params=wrapped_model.parameters(), lr=0.1),\n",
    "    clip_values=(0., 1.), \n",
    "    nb_classes=10, \n",
    "    input_shape=X_test.shape[1:], \n",
    "    device_type=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b94b8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HopSkipJump(targeted=False, norm=2, max_iter=10, max_eval=1000, init_eval=100, init_size=100, curr_iter=0, batch_size=64, verbose=True, )"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the HopSkipJumpAttack  \n",
    "art_attack_white_box = HopSkipJump(classifier=art_white_box_clf, \n",
    "                                   targeted=False, \n",
    "                                   norm=2, \n",
    "                                   max_iter=10,     # Increase this should improve the attack\n",
    "                                   max_eval=1000,   # Same here\n",
    "                                   verbose=True)\n",
    "\n",
    "art_attack_white_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5fdaa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7dadfdc664924a2f565c6bcae9320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] input dtype: torch.float32\n",
      "[DEBUG] input dtype: torch.float64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torch/nn/modules/container.py\", line 22, in forward\n    _5 = getattr(self, \"5\")\n    _6 = getattr(self, \"6\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [1, 1], [1, 1], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 240, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n                self.groups,\n            )\n        return F.conv2d(\n               ~~~~~~~~ <--- HERE\n            input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n        )\nRuntimeError: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate the adversarial examples\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This will generate an error\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_adv_examples = \u001b[43mart_attack_white_box\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/attacks/evasion/hop_skip_jump.py:211\u001b[39m, in \u001b[36mHopSkipJump.generate\u001b[39m\u001b[34m(self, x, y, **kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m         x_adv[ind] = \u001b[38;5;28mself\u001b[39m._perturb(\n\u001b[32m    200\u001b[39m             x=val,\n\u001b[32m    201\u001b[39m             y=y[ind],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m             clip_max=clip_max,\n\u001b[32m    208\u001b[39m         )\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m         x_adv[ind] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perturb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[43minit_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[43madv_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_adv_init\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m y = to_categorical(y, \u001b[38;5;28mself\u001b[39m.estimator.nb_classes)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    224\u001b[39m logger.info(\n\u001b[32m    225\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSuccess rate of HopSkipJump attack: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m     \u001b[32m100\u001b[39m * compute_success(\u001b[38;5;28mself\u001b[39m.estimator, x, y, x_adv, \u001b[38;5;28mself\u001b[39m.targeted, batch_size=\u001b[38;5;28mself\u001b[39m.batch_size),\n\u001b[32m    227\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/attacks/evasion/hop_skip_jump.py:258\u001b[39m, in \u001b[36mHopSkipJump._perturb\u001b[39m\u001b[34m(self, x, y, y_p, init_pred, adv_init, mask, clip_min, clip_max)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03mInternal attack function for one example.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m \u001b[33;03m:return: An adversarial example.\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# First, create an initial adversarial sample\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m initial_sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# If an initial adversarial example is not found, then return the original image\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initial_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/attacks/evasion/hop_skip_jump.py:356\u001b[39m, in \u001b[36mHopSkipJump._init_sample\u001b[39m\u001b[34m(self, x, y, y_p, init_pred, adv_init, mask, clip_min, clip_max)\u001b[39m\n\u001b[32m    349\u001b[39m random_class = np.argmax(\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimator.predict(np.array([random_img]), batch_size=\u001b[38;5;28mself\u001b[39m.batch_size),\n\u001b[32m    351\u001b[39m     axis=\u001b[32m1\u001b[39m,\n\u001b[32m    352\u001b[39m )[\u001b[32m0\u001b[39m]\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random_class != y_p:\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# Binary search to reduce the l2 distance to the original image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     random_img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     initial_sample = random_img, y_p\n\u001b[32m    367\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mFound initial adversarial image for untargeted attack.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/attacks/evasion/hop_skip_jump.py:514\u001b[39m, in \u001b[36mHopSkipJump._binary_search\u001b[39m\u001b[34m(self, current_sample, original_sample, target, norm, clip_min, clip_max, threshold)\u001b[39m\n\u001b[32m    506\u001b[39m interpolated_sample = \u001b[38;5;28mself\u001b[39m._interpolate(\n\u001b[32m    507\u001b[39m     current_sample=current_sample,\n\u001b[32m    508\u001b[39m     original_sample=original_sample,\n\u001b[32m    509\u001b[39m     alpha=alpha,\n\u001b[32m    510\u001b[39m     norm=norm,\n\u001b[32m    511\u001b[39m )\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# Update upper_bound and lower_bound\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m satisfied = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adversarial_satisfactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolated_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    520\u001b[39m lower_bound = np.where(satisfied == \u001b[32m0\u001b[39m, alpha, lower_bound)\n\u001b[32m    521\u001b[39m upper_bound = np.where(satisfied == \u001b[32m1\u001b[39m, alpha, upper_bound)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/attacks/evasion/hop_skip_jump.py:645\u001b[39m, in \u001b[36mHopSkipJump._adversarial_satisfactory\u001b[39m\u001b[34m(self, samples, target, clip_min, clip_max)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[33;03mCheck whether an image is adversarial.\u001b[39;00m\n\u001b[32m    637\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \u001b[33;03m:return: An array of 0/1.\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m samples = np.clip(samples, clip_min, clip_max)\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m preds = np.argmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.targeted:\n\u001b[32m    648\u001b[39m     result = preds == target\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/estimators/classification/classifier.py:75\u001b[39m, in \u001b[36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m0\u001b[39m:\n\u001b[32m     74\u001b[39m     args = \u001b[38;5;28mtuple\u001b[39m(lst)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/estimators/classification/pytorch.py:329\u001b[39m, in \u001b[36mPyTorchClassifier.predict\u001b[39m\u001b[34m(self, x, batch_size, training_mode, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m output = model_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    331\u001b[39m output = output.detach().cpu().numpy().astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/art/estimators/classification/pytorch.py:1165\u001b[39m, in \u001b[36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1162\u001b[39m         result.append(x)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._model, torch.nn.Module):\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1166\u001b[39m     result.append(x)\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mDebugModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[DEBUG] input dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mRuntimeError\u001b[39m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torch/nn/modules/container.py\", line 22, in forward\n    _5 = getattr(self, \"5\")\n    _6 = getattr(self, \"6\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [1, 1], [1, 1], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 240, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/home/securitynik/miniconda3/envs/adversarial_ai/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n                self.groups,\n            )\n        return F.conv2d(\n               ~~~~~~~~ <--- HERE\n            input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n        )\nRuntimeError: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same\n"
     ]
    }
   ],
   "source": [
    "# Generate the adversarial examples\n",
    "# This will generate an error\n",
    "X_adv_examples = art_attack_white_box.generate(x=X_test.numpy(), y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f52ea",
   "metadata": {},
   "source": [
    "Let us now fix the problem above. Above, we created a wrapper for the model to debug the issue. Now that we know what the issue is, let us create a wrapper to fix the issue.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "394f807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new wrapper class for the model\n",
    "class FixedModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dbdbcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedModel(\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "    (1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "    (2): RecursiveScriptModule(original_name=Conv2d)\n",
       "    (3): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "    (4): RecursiveScriptModule(original_name=Flatten)\n",
       "    (5): RecursiveScriptModule(original_name=Linear)\n",
       "    (6): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model for ART\n",
    "art_safe_model = FixedModel(model=loaded_model).to(device)\n",
    "art_safe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64ecb5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "art.estimators.classification.pytorch.PyTorchClassifier(model=ModelWrapper(\n",
       "  (_model): FixedModel(\n",
       "    (model): RecursiveScriptModule(\n",
       "      original_name=Sequential\n",
       "      (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "      (2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (3): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "      (4): RecursiveScriptModule(original_name=Flatten)\n",
       "      (5): RecursiveScriptModule(original_name=Linear)\n",
       "      (6): RecursiveScriptModule(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       "), loss=CrossEntropyLoss(), optimizer=Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "), input_shape=torch.Size([1, 28, 28]), nb_classes=10, channels_first=True, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStdPyTorch(mean=0.0, std=1.0, apply_fit=True, apply_predict=True, device=cuda:0))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify our model \n",
    "# loss function as CrossEntropy Loss\n",
    "# Clip the output between -2 and 2\n",
    "# The original input is between -4 and 4\n",
    "# Set the input shape to the shape of X\n",
    "art_white_box_clf = PyTorchClassifier(model=art_safe_model, \n",
    "                                      loss=torch.nn.CrossEntropyLoss(),\n",
    "                                      optimizer=torch.optim.Adam(params=loaded_model.parameters(), lr=0.1),\n",
    "                                      clip_values=(0., 1.), \n",
    "                                      nb_classes=10, \n",
    "                                      input_shape=X_test.shape[1:], \n",
    "                                      device_type=device)\n",
    "\n",
    "art_white_box_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb8e4549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HopSkipJump(targeted=False, norm=2, max_iter=10, max_eval=100, init_eval=2, init_size=100, curr_iter=0, batch_size=64, verbose=True, )"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the HopSkipJumpAttack  \n",
    "# https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#hopskipjump-attack\n",
    "art_attack_white_box = HopSkipJump(classifier=art_white_box_clf, \n",
    "                                   targeted=False, \n",
    "                                   norm=2, \n",
    "                                   init_eval=2,\n",
    "                                   max_iter=10,     # Increase this should improve the attack\n",
    "                                   max_eval=100,   # Same here\n",
    "                                   verbose=True)\n",
    "\n",
    "art_attack_white_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf9361",
   "metadata": {},
   "source": [
    "I kept the max_iter=10 and max_eval=100 just to ensure the attack is fast in the interest of time. However, feel free to go ahead and increase these numbers. This should give you a better result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa0b0da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea64b913fed47afb5823976632bd9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[[3.84954870e-01, 5.05443588e-02, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 8.77843797e-03, 0.00000000e+00],\n",
       "         [4.66915369e-02, 0.00000000e+00, 3.68113279e-01, ...,\n",
       "          2.65739653e-02, 3.26582164e-01, 0.00000000e+00],\n",
       "         [2.74273127e-01, 2.17970893e-01, 1.26707166e-01, ...,\n",
       "          0.00000000e+00, 3.28005075e-01, 2.17819646e-01],\n",
       "         ...,\n",
       "         [1.86429154e-02, 2.15741485e-01, 2.01144323e-01, ...,\n",
       "          3.97726417e-01, 1.28487036e-01, 4.01828326e-02],\n",
       "         [0.00000000e+00, 1.29454195e-01, 1.22512385e-01, ...,\n",
       "          2.10666940e-01, 1.55011609e-01, 1.31299570e-02],\n",
       "         [4.23655778e-01, 2.80913413e-01, 2.10755900e-03, ...,\n",
       "          1.73176572e-01, 2.60233283e-01, 2.10678130e-01]]],\n",
       "\n",
       "\n",
       "       [[[1.28277063e-01, 7.09191114e-02, 2.26720069e-02, ...,\n",
       "          1.69817694e-02, 3.48678619e-01, 4.64740284e-02],\n",
       "         [1.33095399e-01, 1.90993294e-01, 0.00000000e+00, ...,\n",
       "          2.51156956e-01, 2.40240544e-01, 2.00988397e-01],\n",
       "         [1.73712417e-01, 6.31365031e-02, 1.34062886e-01, ...,\n",
       "          2.47915044e-01, 1.18418731e-01, 0.00000000e+00],\n",
       "         ...,\n",
       "         [0.00000000e+00, 1.71544045e-01, 0.00000000e+00, ...,\n",
       "          3.11063230e-01, 5.16808731e-03, 4.08173278e-02],\n",
       "         [1.24017820e-01, 0.00000000e+00, 7.94789344e-02, ...,\n",
       "          8.30730423e-02, 1.18452057e-01, 2.16665026e-03],\n",
       "         [5.51424772e-02, 2.52497315e-01, 3.08401557e-03, ...,\n",
       "          9.39010233e-02, 1.25271291e-01, 8.92211720e-02]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 0.00000000e+00, 2.05306441e-01, ...,\n",
       "          1.96557015e-01, 4.24976289e-01, 0.00000000e+00],\n",
       "         [1.31844759e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          4.89641018e-02, 2.85967737e-01, 1.54721662e-01],\n",
       "         [1.01385616e-01, 5.52363805e-02, 1.72572851e-01, ...,\n",
       "          2.59269565e-01, 4.71133962e-02, 1.82644963e-01],\n",
       "         ...,\n",
       "         [1.43896788e-01, 1.64211661e-01, 1.94287375e-01, ...,\n",
       "          8.09102133e-02, 2.71901935e-01, 1.97599139e-02],\n",
       "         [1.19734213e-01, 1.88178152e-01, 8.02934915e-02, ...,\n",
       "          1.54219687e-01, 1.77597418e-01, 3.91456671e-02],\n",
       "         [1.31236583e-01, 0.00000000e+00, 3.63842547e-02, ...,\n",
       "          3.29535186e-01, 3.52862895e-01, 1.08413853e-01]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 0.00000000e+00, 6.11854605e-02, ...,\n",
       "          1.87605068e-01, 0.00000000e+00, 1.94446996e-01],\n",
       "         [2.11167052e-01, 1.22934885e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.00886617e-02, 2.15061441e-01],\n",
       "         [8.60278606e-02, 1.63275562e-02, 6.46002144e-02, ...,\n",
       "          6.46963902e-03, 0.00000000e+00, 5.87746724e-02],\n",
       "         ...,\n",
       "         [9.01730955e-02, 1.52325898e-01, 3.76783550e-01, ...,\n",
       "          3.52546237e-02, 1.37171336e-02, 2.26474151e-01],\n",
       "         [1.29850239e-01, 2.13018402e-01, 4.35718268e-01, ...,\n",
       "          2.17742980e-01, 3.41817498e-01, 1.68055832e-01],\n",
       "         [1.46339640e-01, 1.26014158e-01, 1.05715163e-01, ...,\n",
       "          1.95103921e-02, 9.19959508e-03, 1.51230112e-01]]],\n",
       "\n",
       "\n",
       "       [[[2.51986715e-03, 3.07896249e-02, 3.90018849e-03, ...,\n",
       "          4.22720090e-02, 0.00000000e+00, 4.90736961e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 4.78792638e-02, ...,\n",
       "          2.60360669e-02, 9.36659891e-03, 1.62215193e-03],\n",
       "         [2.30948417e-03, 4.66972142e-02, 2.06617508e-02, ...,\n",
       "          4.68266234e-02, 7.05327317e-02, 2.06615252e-04],\n",
       "         ...,\n",
       "         [8.26888308e-02, 3.57325748e-02, 4.05352898e-02, ...,\n",
       "          1.34952003e-02, 6.68191118e-03, 3.27215734e-04],\n",
       "         [0.00000000e+00, 2.41403654e-02, 0.00000000e+00, ...,\n",
       "          1.95657462e-02, 3.68377715e-02, 6.70657977e-02],\n",
       "         [4.70502526e-02, 1.20418065e-03, 5.06641855e-03, ...,\n",
       "          2.81919166e-02, 5.94558986e-03, 2.87096133e-03]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 2.15322264e-02, 1.47162721e-01, ...,\n",
       "          4.83430084e-03, 8.15440863e-02, 7.16188997e-02],\n",
       "         [0.00000000e+00, 5.65470904e-02, 4.70135473e-02, ...,\n",
       "          3.98526527e-02, 2.31925212e-02, 2.10006069e-02],\n",
       "         [8.53546485e-02, 2.13942140e-01, 1.92271322e-01, ...,\n",
       "          1.45105310e-02, 5.80508495e-03, 1.52276903e-01],\n",
       "         ...,\n",
       "         [3.02626759e-01, 2.26571318e-02, 9.71410945e-02, ...,\n",
       "          1.05629481e-01, 0.00000000e+00, 3.15841362e-02],\n",
       "         [6.41281977e-02, 1.82499498e-01, 0.00000000e+00, ...,\n",
       "          5.57787791e-02, 4.70546223e-02, 1.60136428e-02],\n",
       "         [0.00000000e+00, 5.86186573e-02, 3.67548577e-02, ...,\n",
       "          0.00000000e+00, 1.42799407e-01, 1.90051511e-01]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the adversarial examples\n",
    "# As this is untargeted, we simply wishes to flip the class\n",
    "X_adv_examples = art_attack_white_box.generate(x=X_test.numpy(), y=None)\n",
    "X_adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c33cff14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 4, 1, 1, 3, 2, 1], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with the adversarial examples\n",
    "# Convert the adversarial examples to torch tensors\n",
    "X_adv_examples = torch.as_tensor(data=X_adv_examples, dtype=torch.float32, device=device)\n",
    "adv_preds = model(X_adv_examples).argmax(dim=-1)\n",
    "adv_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8411923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the adversarial success\n",
    "# This is our ability to create adversarial samples that flipped the labels of the images\n",
    "adversarial_success = (art_white_box_clf.predict(x=X_adv_examples.detach().cpu()).argmax(axis=-1) != y_test.numpy()).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd2a3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning cuda cache\n"
     ]
    }
   ],
   "source": [
    "# With the training finish clear the GPU cache\n",
    "# Setup the device to work with\n",
    "if torch.cuda.is_available():\n",
    "    # For CUDA GPU\n",
    "    print(f'Cleaning {device} cache')\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    # For Apple devices\n",
    "    print(f'Cleaning {device} cache')\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    # Default to cpu\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2feebd",
   "metadata": {},
   "source": [
    "### Step 5:   \n",
    "Visualize the result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c35b9959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the result of a few samples\n",
    "plt.subplot(241)\n",
    "plt.title('Real Sample')\n",
    "plt.imshow(X_test[0].permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(242)\n",
    "plt.title('Adversarial Sample')\n",
    "plt.imshow(X_adv_examples[0].detach().cpu().permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(243)\n",
    "plt.title('Real Sample')\n",
    "plt.imshow(X_test[1].permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(244)\n",
    "plt.title('Adversarial Sample')\n",
    "plt.imshow(X_adv_examples[1].detach().cpu().permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(245)\n",
    "plt.title('Real Sample')\n",
    "plt.imshow(X_test[2].permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(246)\n",
    "plt.title('Adversarial Sample')\n",
    "plt.imshow(X_adv_examples[2].detach().cpu().permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(247)\n",
    "plt.title('Real Sample')\n",
    "plt.imshow(X_test[3].permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(248)\n",
    "plt.title('Adversarial Sample')\n",
    "plt.imshow(X_adv_examples[3].detach().cpu().permute(dims=(1,2,0)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "209a28ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also confirm that the results of the samples are not the same\n",
    "(X_test[:5] == X_adv_examples.detach().cpu()[:5]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5e940",
   "metadata": {},
   "source": [
    "### Takeaways:   \n",
    "- We leverage HopSkipJumpAttack to create new adversarial samples\n",
    "- In this lab, we had access to the model. \n",
    "- In the real world you may not have access to the model   \n",
    "- We saw how we could mitigate this attack using our firewall. \n",
    "\n",
    "**Keep this lab running at the end. The other two labs are dependent on this infrastructure**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a93e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
