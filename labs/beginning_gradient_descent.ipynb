{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"max-width:20em; height:auto;\" src=\"../graphics/A-Little-Book-on-Adversarial-AI-Cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne   \n",
    "Author Blog: https://www.securitynik.com   \n",
    "Author GitHub: github.com/securitynik   \n",
    "\n",
    "Author Other Books: [   \n",
    "\n",
    "            \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\",   \n",
    "            \n",
    "            \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\"   \n",
    "        ]   \n",
    "\n",
    "\n",
    "This notebook ***(Beginning Gradient Descent.ipynb)*** is part of the series of notebooks From ***A Little Book on Adversarial AI***  A free ebook released by Nik Alleyne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  \n",
    "\n",
    "### Lab Objectives:   \n",
    "- Understanding Backpropagation         \n",
    "- Understand Gradient Descent    \n",
    "- Implement Backpropagation and Gardient Descent   \n",
    "- Validate our work with Pytorch   \n",
    "- Implement a simple model for Gradient Descent      \n",
    "\n",
    "\n",
    "\n",
    "The forward pass being performed on a simple network   \n",
    "<img style=\"max-width:50em; height:auto;\" src=\"../graphics/Forward_Pass_Beginning_Gradient_Descent.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version used:  2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "### Version of key libraries used  \n",
    "print(f'Torch version used:  {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the device to cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to work with\n",
    "# This should ensure if there are accelerators in place, such as Apple backend or CUDA, \n",
    "# we should be able to take advantage of it.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Setting the device to cuda')\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('Setting the device to Apple mps')\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print('Setting the device to CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15, 5.0, 0.92, 0.1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Define my desired initial weight\n",
    "init_weight = 0.15\n",
    "\n",
    "# Define my x - feature\n",
    "x = 5.\n",
    "\n",
    "# define my y - target/label\n",
    "y = 0.92\n",
    "\n",
    "init_weight, x, y, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:   \n",
    "Perform manual prediction   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction manually\n",
    "def forward_pass(w, x):\n",
    "    ''' Make a forward pass on the training data '''\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction\n",
    "y_pred = forward_pass(init_weight, x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:   \n",
    "Define the loss function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss on this pass\n",
    "# Because I have one sample, there is no need to devide by num of samples\n",
    "# So I'm cheating here\n",
    "def my_MSELoss(y_pred, y_true):\n",
    "    ''' Calculates the Mean Squared Error Loss '''\n",
    "    return (y_pred - y_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028900000000000012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the loss on this sample. \n",
    "# Similar to Stochastic Gradient Descent (SGD)\n",
    "mseloss = my_MSELoss(y_pred, y)\n",
    "mseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above done manually, let's confirm with PyTorch that we calculated the forward pass correctly\n",
    "\n",
    "### Step 4:   \n",
    "Create the torch model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/codex/how-to-build-a-pytorch-model-42ae8473a41e\n",
    "# https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    ''' Subclass the torch module '''\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        # Define a layer with 1 input feature and one output target\n",
    "        # Turn off the bias\n",
    "        self.linear = torch.nn.Linear(in_features=1, out_features=1, bias=None)\n",
    "\n",
    "    # Setup the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "# and move it to the device\n",
    "simple_model = SimpleModel().to(device=device)\n",
    "\n",
    "# Add the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(simple_model.parameters(), lr=0.1)\n",
    "\n",
    "# Setup the weights I would like to use\n",
    "# https://www.askpython.com/python-modules/initialize-model-weights-pytorch\n",
    "torch.nn.init.constant_(simple_model.linear.weight.data, init_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.], device='cuda:0'), tensor([0.9200], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the x and y data\n",
    "torch_x = torch.tensor(data=[x], dtype=torch.float32, device=device)\n",
    "torch_y = torch.tensor(data=[y], dtype=torch.float32, device=device)\n",
    "\n",
    "# Print x and y\n",
    "torch_x, torch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0289 | Prediction: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Make the prediction\n",
    "y_pred = simple_model(torch_x)\n",
    "\n",
    "# Calculate the loss\n",
    "loss  = loss_fn(y_pred, torch_y)\n",
    "\n",
    "print(f'loss: {loss.item():.4f} | Prediction: {y_pred.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We have confirmed with PyTorch that the manual calculations for the first epoch (forward pass) was correct. The results match our manual process.   \n",
    "\n",
    "\n",
    "### Step 5:   \n",
    "\n",
    "Time to move on to Gradient Descent via Backpropagation    \n",
    "\n",
    "\n",
    "<img style=\"max-width:50em; height:auto;\" src=\"../graphics/Backward_Pass_Beginning_Gradient_Descent.png\"/>\n",
    "\n",
    "\n",
    "First up, time to calculate the gradients by leveraging backpropagation and the chain rule.\n",
    "\n",
    "Put a link here to the backpropagation post on securitynik.com and Andrej Karpathy post on why you should learn back propagation.   \n",
    "\n",
    "\n",
    "To calculate the gradients, we need to find the partial derivatives of the loss with respect to the weight. Meaning as the weight changes, how does the loss changes.\n",
    "\n",
    "However, when the weight is changed, the prediction will be changed.    \n",
    "When the prediction changes, the loss changes.   \n",
    "Hence, we need to find the partial derivative of the loss with respect to the prediction.  \n",
    "And the derivative of the prediction with respect to the weights.   \n",
    "The **chain rule** can be used to solve this challenge.\n",
    "\n",
    "**dl_dw = dl_dy * dy_dw**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3400000000000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First finding dl_dy - partial derivative of the loss with respect to the prediction\n",
    "# mse loss = (y_pred-y_true) ** 2\n",
    "# To find the partial derivative of the loss with respect to the prediction\n",
    "# it is 2(y_pred - y_true)\n",
    "dl_dy = 2*(0.75-0.92)\n",
    "dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next up, find the partial derivative of \n",
    "# the prediction with respect to the weight\n",
    "# formula for the prediction is y_hat = w*x\n",
    "# hence the partial derivative of dy_dw = x\n",
    "dy_dw = x\n",
    "dy_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7000000000000004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hence dl_dw = dl_dy * dy_dw\n",
    "dl_dw = dl_dy * dy_dw\n",
    "dl_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  \n",
    "Perform gradient descent   \n",
    "\n",
    "Now that the gradient has been calculated, time to update the weight.   \n",
    "To update the weight, the formula is:   \n",
    "**new_weight = old_weight - learning_rate * (dl_dw)**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32000000000000006"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_weight_0 = old_weight_0 - learning_rate * dl_dw\n",
    "new_weight = init_weight - learning_rate*(dl_dw)\n",
    "new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6000000000000003"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the new weight to train the network\n",
    "y_pred = forward_pass(new_weight, x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46240000000000037"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "my_MSELoss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the process above should be done via a loop, rather than individually.  Before coding this up, time to verify with PyTorch that the gradients are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the init_weight: 0.1500\n",
      "This is the prediction: 0.75\n",
      "This is the loss: 0.0289\n"
     ]
    }
   ],
   "source": [
    "# First convert the init_weight to a torch tensor \n",
    "# By setting requires.grad_(True), we are saying this parameter can be updated/trained\n",
    "init_weight = torch.tensor(data=init_weight, device=device).requires_grad_(True)\n",
    "print(f'This is the init_weight: {init_weight:.4f}')\n",
    "\n",
    "# Make the prediction  \n",
    "y_hat = torch.multiply(init_weight, torch_x)\n",
    "print(f'This is the prediction: {y_hat.detach().cpu().numpy().item()}')\n",
    "\n",
    "# Calculate the loss  \n",
    "loss = F.mse_loss(input=y_hat, target=torch_y)\n",
    "print(f'This is the loss: {loss:.4f}')\n",
    "\n",
    "# Perform backpropagation \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.7000, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Retrieve the gradients for the loss with respect to the weight\n",
    "init_weight.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:   \n",
    "Things are looking good so far. Build a Torch model to automate that process  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 0.02890000492334366\n",
      "Epoch 1: loss: 0.4624001681804657\n"
     ]
    }
   ],
   "source": [
    "# Confirming with PyTorch\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    ''' Subclass the torch module '''\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        # Define a layer with 1 input feature and one output target\n",
    "        # Turn off the bias\n",
    "        self.linear = torch.nn.Linear(in_features=1, out_features=1, bias=None)\n",
    "\n",
    "    # Setup the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Define the model \n",
    "simple_model = SimpleModel().to(device=device)\n",
    "optimizer = torch.optim.SGD(simple_model.parameters(), lr=0.1)\n",
    "torch.nn.init.constant_(simple_model.linear.weight, init_weight)\n",
    "\n",
    "torch_loss = []\n",
    "\n",
    "# Train the model for two epochs\n",
    "for i in range(0, 2):\n",
    "    y_pred = simple_model(torch_x)\n",
    "    loss = my_MSELoss(y_pred, torch_y)\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch_loss.append(loss.item())\n",
    "    print(f'Epoch {i}: loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also confirms the manual process and the PyTorch Gradient calculation \n",
    "# Time to now code up our own and close this notebook off.\n",
    "# Quick and dirty just for this purpose\n",
    "def my_gradient(w, x, y):\n",
    "    for i in range(0, 2):\n",
    "        y_pred = forward_pass(w, x)\n",
    "        loss = (y_pred - y) ** 2\n",
    "        dl_dw = 2*(y_pred - y) * x\n",
    "        w = w - 0.1 *(dl_dw)\n",
    "        print(f'Epoch: {i}: Prediction: {y_pred} : Loss:{loss}' )\n",
    "\n",
    "    #return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: Prediction: 0.75 : Loss:0.02890000492334366\n",
      "Epoch: 1: Prediction: 1.6000001430511475 : Loss:0.4624001681804657\n"
     ]
    }
   ],
   "source": [
    "# call the function and confirm or working.\n",
    "my_gradient(init_weight, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning cuda cache\n"
     ]
    }
   ],
   "source": [
    "# With the training finish clear the GPU cache\n",
    "# Setup the device to work with\n",
    "if torch.cuda.is_available():\n",
    "    # For CUDA GPU\n",
    "    print(f'Cleaning {device} cache')\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    # For Apple devices\n",
    "    print(f'Cleaning {device} cache')\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    # Default to cpu\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this lab        \n",
    "### Lab Takeaways:  \n",
    "- We learnt about Gradient Descent \n",
    "- We implemented Gradient Descent   \n",
    "- We learned about backpropagation    \n",
    "- We implemented backpropagation    \n",
    "- We saw how frameworks such as Pytorch, helps to make this process extremely easier.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
