{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c22c24",
   "metadata": {},
   "source": [
    "<img style=\"max-width:20em; height:auto;\" src=\"../graphics/A-Little-Book-on-Adversarial-AI-Cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a44d5",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne   \n",
    "Author Blog: https://www.securitynik.com   \n",
    "Author GitHub: github.com/securitynik   \n",
    "\n",
    "Author Other Books: [   \n",
    "\n",
    "            \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\",   \n",
    "            \n",
    "            \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\"   \n",
    "        ]   \n",
    "\n",
    "\n",
    "This notebook ***(understanding_transfer_learning.ipynb)*** is part of the series of notebooks From ***A Little Book on Adversarial AI***  A free ebook released by Nik Alleyne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd34f28",
   "metadata": {},
   "source": [
    "### Understanding Transfer Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211b9e3",
   "metadata": {},
   "source": [
    "### Lab Objectives:   \n",
    "- Understanding Transfer Learning  \n",
    "- Creating a classifier from transfer learning \n",
    "- Understand what freezing the model's parameters mean \n",
    "\n",
    "  \n",
    "### Steps 1:  \n",
    "Get the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578be570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries   \n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5521e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version used:  2.7.1+cu128\n",
      "Torchinfo version used:  1.8.0\n"
     ]
    }
   ],
   "source": [
    "### Version of key libraries used  \n",
    "print(f'Torch version used:  {torch.__version__}')\n",
    "print(f'Torchinfo version used:  {torchinfo.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22381b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the device to cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to work with\n",
    "# This should ensure if there are accelerators in place, such as Apple backend or CUDA, \n",
    "# we should be able to take advantage of it.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Setting the device to cuda')\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('Setting the device to Apple mps')\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print('Setting the device to CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b50152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ResNet                                   --\n",
       "├─Conv2d: 1-1                            9,408\n",
       "├─BatchNorm2d: 1-2                       128\n",
       "├─ReLU: 1-3                              --\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─BasicBlock: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  36,864\n",
       "│    │    └─BatchNorm2d: 3-2             128\n",
       "│    │    └─ReLU: 3-3                    --\n",
       "│    │    └─Conv2d: 3-4                  36,864\n",
       "│    │    └─BatchNorm2d: 3-5             128\n",
       "│    └─BasicBlock: 2-2                   --\n",
       "│    │    └─Conv2d: 3-6                  36,864\n",
       "│    │    └─BatchNorm2d: 3-7             128\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  36,864\n",
       "│    │    └─BatchNorm2d: 3-10            128\n",
       "├─Sequential: 1-6                        --\n",
       "│    └─BasicBlock: 2-3                   --\n",
       "│    │    └─Conv2d: 3-11                 73,728\n",
       "│    │    └─BatchNorm2d: 3-12            256\n",
       "│    │    └─ReLU: 3-13                   --\n",
       "│    │    └─Conv2d: 3-14                 147,456\n",
       "│    │    └─BatchNorm2d: 3-15            256\n",
       "│    │    └─Sequential: 3-16             8,448\n",
       "│    └─BasicBlock: 2-4                   --\n",
       "│    │    └─Conv2d: 3-17                 147,456\n",
       "│    │    └─BatchNorm2d: 3-18            256\n",
       "│    │    └─ReLU: 3-19                   --\n",
       "│    │    └─Conv2d: 3-20                 147,456\n",
       "│    │    └─BatchNorm2d: 3-21            256\n",
       "├─Sequential: 1-7                        --\n",
       "│    └─BasicBlock: 2-5                   --\n",
       "│    │    └─Conv2d: 3-22                 294,912\n",
       "│    │    └─BatchNorm2d: 3-23            512\n",
       "│    │    └─ReLU: 3-24                   --\n",
       "│    │    └─Conv2d: 3-25                 589,824\n",
       "│    │    └─BatchNorm2d: 3-26            512\n",
       "│    │    └─Sequential: 3-27             33,280\n",
       "│    └─BasicBlock: 2-6                   --\n",
       "│    │    └─Conv2d: 3-28                 589,824\n",
       "│    │    └─BatchNorm2d: 3-29            512\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─Conv2d: 3-31                 589,824\n",
       "│    │    └─BatchNorm2d: 3-32            512\n",
       "├─Sequential: 1-8                        --\n",
       "│    └─BasicBlock: 2-7                   --\n",
       "│    │    └─Conv2d: 3-33                 1,179,648\n",
       "│    │    └─BatchNorm2d: 3-34            1,024\n",
       "│    │    └─ReLU: 3-35                   --\n",
       "│    │    └─Conv2d: 3-36                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-37            1,024\n",
       "│    │    └─Sequential: 3-38             132,096\n",
       "│    └─BasicBlock: 2-8                   --\n",
       "│    │    └─Conv2d: 3-39                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-40            1,024\n",
       "│    │    └─ReLU: 3-41                   --\n",
       "│    │    └─Conv2d: 3-42                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            1,024\n",
       "├─AdaptiveAvgPool2d: 1-9                 --\n",
       "├─Linear: 1-10                           513,000\n",
       "=================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up a pretrained model\n",
    "# In this case, we will use the resnet18\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html \n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "\n",
    "# Looking at the model summary\n",
    "torchinfo.summary(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbf4487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a closer look at the layer\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac17b72",
   "metadata": {},
   "source": [
    "The model needs to be transferred to **eval** mode.  This is important as can be seen above, the model has **BatchNorm2d** layers. BatchNorm2d operates different at training time versus testing time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6852f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e39fa5",
   "metadata": {},
   "source": [
    "### Step 2:  \n",
    "Review the model parameters for layer *model.conv1.weight* .  Also verify that it is trainable, via its **requires_grad** parameter. If *True* it means its parameters can be updated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fdad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight state BEFORE training. Trainable?: True\n",
      "model.conv1.weight parameters BEFORE training: \n",
      "tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
      "        [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
      "        [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
      "        [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
      "        [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
      "        [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
      "        [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the layer information\n",
    "print(f'model.conv1.weight state BEFORE training. Trainable?: {model.conv1.weight.requires_grad}')\n",
    "\n",
    "# Now that we have the model, lets look at layer 1 and its parameters\n",
    "# Take the first layer and get a look at the weights\n",
    "print(f'model.conv1.weight parameters BEFORE training: \\n{model.conv1.weight[0, 0, :, :]}')\n",
    "\n",
    "# Let us also capture this as a variable\n",
    "conv1_weights_before = model.conv1.weight[0, 0, :, :].detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b011c",
   "metadata": {},
   "source": [
    "Let us take layer 2 *conv2 layer* in tbe *BasicBlock* and grab its parameters. Ultimately, we want to freeze all layers except this one.  This one was chosen randomly simply for explaining Transfer Learning.  No other reason.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b99589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer2[0].conv2.weight state BEFORE training. Trainable?: True\n",
      "model.layer2[0].conv2.weight BEFORE training: \n",
      "tensor([[-0.0074, -0.0098,  0.0028],\n",
      "        [-0.0108,  0.0258,  0.0455],\n",
      "        [-0.0272,  0.0053,  0.0132]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'model.layer2[0].conv2.weight state BEFORE training. Trainable?: {model.layer2[0].conv2.weight.requires_grad}')\n",
    "\n",
    "print(f'model.layer2[0].conv2.weight BEFORE training: \\n{model.layer2[0].conv2.weight[0, 0, :, :]}')\n",
    "\n",
    "# Capture this as a variable to compare with the trained version later\n",
    "conv2_weights_before = model.layer2[0].conv2.weight[0, 0, :, :].detach().clone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e66098",
   "metadata": {},
   "source": [
    "What we want during transfer learning, is to freeze these weights. We will leverage the features learned from the previous training process. \n",
    "\n",
    "Freezing means that when we train the network, we don't wish to update these parameters (weights and biases). In the case of Fine tuning, we would retrain some of these weights.  \n",
    "\n",
    "Let's keep it simple and stick with transfer learning. We will freeze all layers, except model.layer2[0].conv2.weight. \n",
    "\n",
    "When this is finished, we should see that the weight for model.conv1 remains the same, while the weight for model.layer2[0].conv2.weight will change.\n",
    "\n",
    "To freeze the layers, we set **requires grad to False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe30dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a53b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.conv1.weight - AFTER FREEZING but BEFORE training. Trainable?: False\n",
      "Layer: model.layer2[0].conv2.weight AFTER FREEZING but BEFORE training. Trainable?: False\n"
     ]
    }
   ],
   "source": [
    "# Confirm that gradient is no longer required on the two layers we targeted above\n",
    "print(f'Layer: model.conv1.weight - AFTER FREEZING but BEFORE training. Trainable?: {model.conv1.weight.requires_grad}')\n",
    "print(f'Layer: model.layer2[0].conv2.weight AFTER FREEZING but BEFORE training. Trainable?: {model.layer2[0].conv2.weight.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd2321",
   "metadata": {},
   "source": [
    "Above confirms that by using .requires_grad = False, we were able to freeze the layers, thus making them untrainable. \n",
    "\n",
    "Let us now unfreeze model.layer2[0].conv2.weight to see the updates.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7648360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer2[0].conv2.weight AFTER FREEZING but BEFORE training: True\n"
     ]
    }
   ],
   "source": [
    "# Unfreezing the model.layer2[0].conv2 layer\n",
    "model.layer2[0].conv2.weight.requires_grad = True\n",
    "\n",
    "print(f'model.layer2[0].conv2.weight AFTER FREEZING but BEFORE training: {model.layer2[0].conv2.weight.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb2ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer of the network BEFORE modification: \n",
      "Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Let's capture the information from the last layer of the network\n",
    "print(f'Last layer of the network BEFORE modification: \\n{model.fc}')\n",
    "\n",
    "# Capture the number of neurons coming into and exiting the final layer\n",
    "final_layer_in_features = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe10cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer of the network AFTER modification: \n",
      "Linear(in_features=512, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Freeze Torch's random number generator\n",
    "torch.manual_seed(seed=10)\n",
    "\n",
    "# Let us create our own final layer\n",
    "# Replace the output layer with 1 neuron\n",
    "model.fc = nn.Linear(in_features=final_layer_in_features, out_features=1, bias=True)\n",
    "\n",
    "# Revisit the final layer of the ne|twork to confirm our model architecture has been updated\n",
    "print(f'Last layer of the network AFTER modification: \\n{model.fc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddef923",
   "metadata": {},
   "source": [
    "The network requires input of shape 3x224x224 https://pytorch.org/hub/pytorch_vision_resnet/. Let's create 10 random samples to use. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. \n",
    "\n",
    "We are only concerned with seeing how the parameters change state. As a result, getting a true image is of no importance. We can simply replace this approach with a real dataset if we wanted to solve a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd1c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 1., 1.,  ..., 1., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 0.,  ..., 1., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 1.],\n",
       "         ...,\n",
       "         [0., 1., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 1., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze the random number generator\n",
    "torch.manual_seed(seed=10)\n",
    "\n",
    "# Create six random samples \n",
    "# Place the samples on the device  \n",
    "X = torch.randint(low=0, high=2, size=(6,3,224,224), dtype=torch.float, device=device)\n",
    "\n",
    "# Just a peak of a sample if you are interested\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8934ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a few labels to match those samples\n",
    "y_truth = torch.tensor(data=[[0], [1], [1], [0], [1], [0]], dtype=torch.float32, device=device)\n",
    "y_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079850be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c73e52a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 \t loss: 0.7258352041244507\n",
      "Epoch: 2/5 \t loss: 1.1715543270111084\n",
      "Epoch: 3/5 \t loss: 4.444774150848389\n",
      "Epoch: 4/5 \t loss: 3.578293800354004\n",
      "Epoch: 5/5 \t loss: 3.6656856536865234\n"
     ]
    }
   ],
   "source": [
    "# Set the manual seed \n",
    "torch.manual_seed(seed=10)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Let's prepare to train the model\n",
    "# First define a loss function\n",
    "# Here we use with logits as we don't have an activation function at the end of the network\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer for Gradient Descent\n",
    "# Setting a relatively high learning rate here jus to speed up the learning process\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=.1)\n",
    "\n",
    "# Define a number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Train the network now for 5 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients to prevent them from accumulating\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # calculate the loss on the model predictions vs the ground truth\n",
    "    loss = loss_fn(input=model(X.to(device)), target=y_truth)\n",
    "    \n",
    "    # Perform backpropagation on the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Updates the parameters with gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} \\t loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7518548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0073],\n",
       "        [0.0079],\n",
       "        [0.0081],\n",
       "        [0.0088],\n",
       "        [0.0073],\n",
       "        [0.0081]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get some sample predictions\n",
    "# Note the sigmoid activation function\n",
    "# This is needed as the model output the raw values as the logits\n",
    "F.sigmoid(model(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260bc48",
   "metadata": {},
   "source": [
    "Now that the model has been trained and we know we can make predictions. Time to find out if our parameters have been updated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5dcb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight AFTER FREEZING and AFTER training: False\n",
      "model.conv1.weight: Original weights: \n",
      "tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
      "        [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
      "        [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
      "        [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
      "        [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
      "        [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
      "        [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]],\n",
      "       device='cuda:0')\n",
      "\n",
      "model.conv1.weight AFTER FREEZING and AFTER training: \n",
      "tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
      "        [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
      "        [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
      "        [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
      "        [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
      "        [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
      "        [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Confirm the layer is still frozen\n",
    "# Meaning it is not still trainable\n",
    "print(f'model.conv1.weight AFTER FREEZING and AFTER training: {model.conv1.weight.requires_grad}')\n",
    "\n",
    "# Capture the variable again\n",
    "conv1_weights_after_training = model.conv1.weight[0, 0, :, :].detach().clone()\n",
    "\n",
    "# Print the original weights\n",
    "print(f'model.conv1.weight: Original weights: \\n{conv1_weights_before}')\n",
    "\n",
    "# print the weights after training\n",
    "print(f'\\nmodel.conv1.weight AFTER FREEZING and AFTER training: \\n{model.conv1.weight[0, 0, :, :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b49d12d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare conv1 before and after training\n",
    "# We expect this to be unchanged as is above\n",
    "(conv1_weights_before == conv1_weights_after_training).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "836e3fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer2[0].conv2.weight AFTER FREEZING and AFTER training. Trainable?: True\n",
      "\n",
      " Weights before: \n",
      "tensor([[-0.0074, -0.0098,  0.0028],\n",
      "        [-0.0108,  0.0258,  0.0455],\n",
      "        [-0.0272,  0.0053,  0.0132]], device='cuda:0')\n",
      "\n",
      "model.layer2[0].conv2.weight AFTER FREEZING and AFTER training : \n",
      "tensor([[-0.0080, -0.0106,  0.0022],\n",
      "        [-0.0117,  0.0250,  0.0443],\n",
      "        [-0.0277,  0.0047,  0.0124]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Look at the model.layer2[0].conv2 layer\n",
    "print(f'model.layer2[0].conv2.weight AFTER FREEZING and AFTER training. Trainable?: {model.layer2[0].conv2.weight.requires_grad}')\n",
    "\n",
    "# Capture the new weights, now that the model has been trained\n",
    "conv2_weights_after_training = model.layer2[0].conv2.weight[0, 0, :, :].detach().clone()\n",
    "\n",
    "print(f'\\n Weights before: \\n{conv2_weights_before}')\n",
    "\n",
    "print(f'\\nmodel.layer2[0].conv2.weight AFTER FREEZING and AFTER training : \\n{model.layer2[0].conv2.weight[0, 0, :, :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5acb034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the untrained and trained weights equal?        \n",
      "tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Looking at things from a different perspective.  \n",
    "print(f'Are the untrained and trained weights equal? \\\n",
    "       \\n{(conv2_weights_before == conv2_weights_after_training)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7340f5a",
   "metadata": {},
   "source": [
    "### Lab Takeaways:\n",
    "- We learned about transfer learning\n",
    "- In transfer learning, we frooze the layers of the trained model and modified the final layer\n",
    "- Alternatively, we could have added additional layers to make a larger *head* if we wanted   \n",
    "- With more layers, it should be easier for the new model to learn the pattern of the data.   \n",
    "- How you approach the additional of additional layers will depend on the problem you are solving.\n",
    "\n",
    "#### Fine Tuning  \n",
    "In fine tuning, we retrain the majority of the layers in the network at a slower learning rate. We may also introduce learning rate decay, which further slows down the training as we progress over the epochs. \n",
    "\n",
    "Additional Reference:   \n",
    "- https://www.geeksforgeeks.org/top-pre-trained-models-for-image-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
